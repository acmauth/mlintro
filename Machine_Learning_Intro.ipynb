{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Intro",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lIXRLt3iB9Hn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Intro #\n",
        "\n",
        "Let's go through an Regression problem. The main steps we are going to take is: \n",
        "\n",
        "1. Load the data\n",
        "2. Discover and visualize the data to gain insights\n",
        "3. Prepare data for machine learning algorithms\n",
        "4. Select an algorithm and train a model\n",
        "5. Validate our model\n",
        "\n",
        "The dataset we are going to use is the [California Housing Dataset](https://github.com/ageron/handson-ml/tree/master/datasets/housing) which contains data drawn from the 1990 U.S. Census. \n",
        "\n",
        "The first task you are asked to perform is to build a model of housing prices in California using the California census data. This data has metrics such as the **population**, **median income**, **median housing\n",
        "price**, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short.\n",
        "\n",
        "\n",
        "Your model should learn from this data and be able to **predict the median housing price in any district, given all the other metrics**."
      ]
    },
    {
      "metadata": {
        "id": "tQovSONHCHqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the workspace ##\n",
        "\n",
        "If you are working *offline* you can create a conda environment now with `conda create -name linear` and install the libraries needed with `pip install matplotlib numpy pandas scipy scikit-learn`"
      ]
    },
    {
      "metadata": {
        "id": "0iudJYF5B1Nd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Several imports that will be needed - Check them before starting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3e9hmwdACTqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loading ##\n",
        "\n",
        "We are loading the data using pandas. Write a small function for this, we are gonna need it later. We take a quick look into the data using pandas."
      ]
    },
    {
      "metadata": {
        "id": "rhmoliyVCRES",
        "colab_type": "code",
        "outputId": "26f3adb2-b474-4131-ea05-1694715564c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"housing.csv\"\n",
        "\n",
        "def load_data_csv(csv_path=DATASET_PATH):\n",
        "  return pd.read_csv(csv_path)\n",
        "\n",
        "housing = load_data_csv()\n",
        "housing.head() # Top 5 rows "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f99f059eda94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhousing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Top 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f99f059eda94>\u001b[0m in \u001b[0;36mload_data_csv\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhousing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'housing.csv' does not exist: b'housing.csv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sXjmSegnCuW6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data discovery ##"
      ]
    },
    {
      "metadata": {
        "id": "Q9M7D25QCx6A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "housing.info() # Quick description of the data (types, columns, entries etc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7shvn9w7C1oV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are 20,640 instances in the dataset. Notice that the total_bed\n",
        "rooms attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. We will need to take care of this later. \n",
        "\n",
        "Also, all attributes are number except the oceant_proximity field. It is type object, since we loaded from csv, it is a text field. Take a look back at the top5 rows."
      ]
    },
    {
      "metadata": {
        "id": "cyIjFmCYC7Bk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's find all about this field\n",
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDRTV6bxC9Tn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some further information about all the fields\n",
        "housing.describe()\n",
        "\n",
        "# STD = Standard Deviation, 25% = 25th percentile, 50% = median , 75% = 75th percentile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QxDB7JdHDDK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data visualization ##\n",
        "\n",
        "### General visualization ###\n",
        "\n",
        "Plotting histograms for each numerical value also helps us understand the data. A histogram shows the number of instances\n",
        "(on the vertical axis) that have a given value range (on the horizontal axis)."
      ]
    },
    {
      "metadata": {
        "id": "leBbHwyQDFxd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # Necessary import\n",
        "housing.hist(bins=50, figsize=(20,15)) #Using like this plots for each \n",
        "#nummerical value, we choose the number of bins - the number of segments -.\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYoaQPUZDN8p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We notice:\n",
        " * Some have been capped (median income at 15, median house age, median house value)\n",
        " * Different scales\n",
        " * Tail heavy histograms (they extend much farther to the right of the median than to the left)"
      ]
    },
    {
      "metadata": {
        "id": "Crl_B1hoDQ3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing Geographical Data ##\n",
        "\n",
        "Since there is geographical information (latitude and longitude), it is a good idea to\n",
        "create a scatterplot of all districts to visualize the data. \n",
        "\n",
        "Here is California in the map to help us understand our data better.\n",
        "\n",
        "<img src=\"http://www.orangesmile.com/common/img_city_maps/california-state-map-3.jpg \" alt=\"california\" width=\"400\"/>"
      ]
    },
    {
      "metadata": {
        "id": "bOzOOdp_DUqj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\") # Scatter with longitute in the x axis and the latitude in th y axis. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJPl-ASBDXXX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points."
      ]
    },
    {
      "metadata": {
        "id": "MsoRQNp3DZt2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) # Scatter with longitute in the x axis and the latitude in th y axis. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uaqil19sDbuG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now see the high density areas (around the main cities of california).\n",
        "\n",
        "More generally, our brains are very good at spotting patterns on pictures, but you may need to play around with visualization parameters to make the patterns stand out.\n",
        "\n",
        "Let's include the housing prices. The radius of each circle represents the district’s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices).\n"
      ]
    },
    {
      "metadata": {
        "id": "cXcQEgXVDeS4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
        "s=housing[\"population\"]/100, label=\"population\",\n",
        "c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
        ")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1lmRaUCmDqLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare the data for Machine Learning Algorithms #\n",
        "\n",
        "Instead of just doing this manually, you should write functions to do that, for several good reasons:\n",
        " \n",
        " * Reproduce the transformations on any dataset\n",
        " * Build a library of transformations functions\n",
        " * Easily try various transformations\n",
        " \n",
        " Let's work with our training set and separate the features (input values) from the labels (output values - median house value)"
      ]
    },
    {
      "metadata": {
        "id": "X7B4eZyZDtEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "x = housing.drop(\"median_house_value\", axis=1) # Creates a copy without the specified column\n",
        "y = housing[\"median_house_value\"].copy() # Copies the column to the specified variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCKiUiweFj3A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning ##\n",
        "\n",
        "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:\n",
        "\n",
        "1. Get rid of the corresponding districts.\n",
        "2. Get rid of the whole attribute.\n",
        "3. Set the values to some value (zero, the mean, the median, etc.).\n",
        "\n",
        "In cases where you have more columns with missing values you can easily use the [Simple Imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) from the scikit learn library. \n"
      ]
    },
    {
      "metadata": {
        "id": "RmSTvA6DFnfv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#housing.dropna(subset=[\"total_bedrooms\"]) # option 1\n",
        "# housing.drop(\"total_bedrooms\", axis=1) # option 2\n",
        "median = x[\"total_bedrooms\"].median() # Get the median of this column\n",
        "x[\"total_bedrooms\"].fillna(median) # option 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI7nwYF_FrwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Handling Text and Categorical Attributes ##\n",
        "\n",
        "The ocean_proximity fields is a text attribute. Most machine learning algorithms prefer to work with numbers, so is it better to convert these text labels to numbers."
      ]
    },
    {
      "metadata": {
        "id": "cj2r01QdFuuf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "housing_cat = x[\"ocean_proximity\"]\n",
        "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
        "print(housing_cat_encoded)\n",
        "print(encoder.classes_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HqhXpS80FxNd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each category is mapped to a value ex. <1H OCEAN is mapped  to 0, INLDAND is mapped to 1 etc.\n",
        "\n",
        "One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. Obviously this is not the case (for example, categories 0 and 4 are more similar than categories 0 and 1).\n",
        "\n",
        "To fix this we create one binary category for each attribute. One attribute is equal to 1 when the category matched the ocean proximity of the district and 0 to all other attributes.\n",
        "\n",
        "\n",
        "Scikit-Learn provides a [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) encoder to convert integer categorical values into one-hot vectors. Let’s encode the categories as one-hot vectors. Note that fit_transform() expects a 2D array, but housing_cat_encoded is a 1D array, so we need to reshape it."
      ]
    },
    {
      "metadata": {
        "id": "AEVnYzMWF0b0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
        "housing_cat_1hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gumEUtX8F3hO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding we get a matrix with thousands of columns, and the matrix is full of zeros except for one 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. You can use it mostly like a normal 2D array,19 but if you really want to convert it to a (dense) NumPy array, just call the toarray() method."
      ]
    },
    {
      "metadata": {
        "id": "vMCZYSqWF2dd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "housing_cat_1hot.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-u_WTb9iGXxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pipelines ##\n",
        "\n",
        "You can combine all the transformations into one [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). The names can be anything you like."
      ]
    },
    {
      "metadata": {
        "id": "wqI-trPJGZjz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "housing_num = x.drop(\"ocean_proximity\", axis=1) # Get only the numerical values\n",
        "\n",
        "\n",
        "# Set up the pipeline\n",
        "num_pipeline = Pipeline([\n",
        "('imputer', SimpleImputer(strategy=\"median\")),\n",
        "('normalization', MinMaxScaler()),\n",
        "])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
        "\n",
        "pd.DataFrame(housing_num_tr).head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwKo-AwAGcx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's join the numerical values with the categorical. We do that with the [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
      ]
    },
    {
      "metadata": {
        "id": "npbtu8IDGgl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Specify the transformer and the columns to affect. You also use pipelines as transformer.\n",
        "full_pipeline = ColumnTransformer([\n",
        "    (\"num_pipeline\", num_pipeline, x.columns[:-1]),\n",
        "    ('label_binarizer', OneHotEncoder(),[x.columns[-1]])\n",
        "])\n",
        "housing_prepared = full_pipeline.fit_transform(x)\n",
        "pd.DataFrame(housing_prepared).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TdLAkdzSGf7y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Select and Train a Model  ##\n",
        "\n",
        "Following all these steps made the process of training a model and evaluating it much simpler. In this sections will train a regression using the sklearn method of LinearRegression."
      ]
    },
    {
      "metadata": {
        "id": "v-W0Mj8bG96t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (sklearn) ###\n",
        "\n",
        "We train a linear regression model using the scikit learn library."
      ]
    },
    {
      "metadata": {
        "id": "UQg9agyYGnH8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sBODOc85G6NE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's try out on few instances from the training set\n",
        "some_data = x.iloc[:5] # Choosing some data\n",
        "some_labels = y.iloc[:5] # Don't forget to also get the labels\n",
        "some_data_prepared = full_pipeline.transform(some_data) # Transform thoses data with the pipeline\n",
        "print(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\n",
        "print(\"Labels:\\t\\t\", list(some_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UIEiiIv2FFCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation ##"
      ]
    },
    {
      "metadata": {
        "id": "l86SrB0IHDp1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predictions doesn't seem very accurate. Let's measure this regression model’s Root Mean Square Error (RMSE)"
      ]
    },
    {
      "metadata": {
        "id": "_iZofMnMHGlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared) # Taking the predictions on the training set\n",
        "lin_mse = mean_squared_error(y, housing_predictions) # Computing mean squared error\n",
        "lin_rmse = np.sqrt(lin_mse) # Taking the root\n",
        "lin_rmse"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}